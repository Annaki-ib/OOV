{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITXPzPh279ni",
        "outputId": "8a1d766e-d6c3-45c3-d05b-05109604352f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: conllu in /usr/local/lib/python3.11/dist-packages (6.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qTIVEf6t5ASC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from conllu import parse_incr\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "#############################\n",
        "# CONSTANTES ET VOCABULAIRES\n",
        "#############################\n",
        "\n",
        "# Pour les caractères (input)\n",
        "PAD_CHAR = \"<pad>\"   # indice 0\n",
        "UNK_CHAR = \"<unk>\"   # indice 1\n",
        "ESP_CHAR = \"<esp>\"   # symbole servant à marquer la frontière entre les mots (par exemple, espace)\n",
        "# On fixera manuellement les indices pour ces tokens spéciaux :\n",
        "PAD_ID = 0\n",
        "UNK_ID = 1\n",
        "ESP_ID = 2\n",
        "\n",
        "def build_char_vocab(data_file, min_freq=1):\n",
        "    \"\"\"\n",
        "    Construit un vocabulaire des caractères à partir des formes (form) des tokens dans le fichier.\n",
        "    On compte chaque caractère apparaissant dans les mots.\n",
        "    Les tokens spéciaux PAD, UNK et ESP sont ajoutés par défaut.\n",
        "    \"\"\"\n",
        "    counter = Counter()\n",
        "    with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        for sentence in parse_incr(f):\n",
        "            for token in sentence:\n",
        "                word = token[\"form\"]\n",
        "                for ch in word:\n",
        "                    counter[ch] += 1\n",
        "    # Vocabulaire initial avec tokens spéciaux (indices fixés)\n",
        "    vocab = {PAD_CHAR: PAD_ID, UNK_CHAR: UNK_ID, ESP_CHAR: ESP_ID}\n",
        "    idx = 3\n",
        "    for ch, freq in counter.items():\n",
        "        if freq >= min_freq and ch not in vocab:\n",
        "            vocab[ch] = idx\n",
        "            idx += 1\n",
        "    return vocab\n",
        "\n",
        "def build_number_vocab(data_file):\n",
        "    \"\"\"\n",
        "    Construit le vocabulaire pour le trait morphologique Number.\n",
        "    Les tokens de sortie seront :\n",
        "      - \"<PAD>\" pour le padding (indice 0)\n",
        "      - \"<N/A>\" pour les mots qui n'ont pas de trait Number (indice 1)\n",
        "      - puis les différentes valeurs observées (p.ex. \"Sing\", \"Plur\", etc.)\n",
        "    \"\"\"\n",
        "    vocab = {\"<PAD>\": 0, \"<N/A>\": 1}\n",
        "    idx = 2\n",
        "    with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        for sentence in parse_incr(f):\n",
        "            for token in sentence:\n",
        "                feats = token.get(\"feats\")\n",
        "                if feats and \"Number\" in feats:\n",
        "                    number = feats[\"Number\"]\n",
        "                    if number not in vocab:\n",
        "                        vocab[number] = idx\n",
        "                        idx += 1\n",
        "    return vocab\n",
        "\n",
        "########################################\n",
        "# ENCODAGE D'UNE PHRASE (CARACTÈRES)\n",
        "########################################\n",
        "\n",
        "def encode_sentence(sentence, char_vocab, num_vocab, max_c, max_w):\n",
        "    \"\"\"\n",
        "    Pour une phrase (liste de tokens du fichier CoNLL-U) :\n",
        "      - Crée in_enc : séquence d'indices de caractères commençant par <pad>\n",
        "      - Crée ends : liste des positions (indices dans in_enc) correspondant à la fin de chaque mot\n",
        "      - Crée out_enc : pour chaque mot, l'indice associé à la valeur du trait Number,\n",
        "        ou \"<N/A>\" si le trait est absent.\n",
        "    On ajoute après chaque mot le token <esp> pour marquer la frontière (mais on enregistre la fin du mot avant <esp>).\n",
        "    Enfin, on tronque si la séquence dépasse max_c (pour in_enc) ou si le nombre de mots dépasse max_w.\n",
        "    \"\"\"\n",
        "    in_enc = [char_vocab[PAD_CHAR]]  # on démarre avec <pad> (optionnel, selon la doc)\n",
        "    ends = []\n",
        "    out_enc = []\n",
        "    for token in sentence:\n",
        "        word = token[\"form\"]\n",
        "        # Encoder chaque caractère du mot\n",
        "        for ch in word:\n",
        "            in_enc.append(char_vocab.get(ch, UNK_ID))\n",
        "        # La position de fin du mot est l'index du dernier caractère ajouté\n",
        "        ends.append(len(in_enc) - 1)\n",
        "        # Ajouter le token de séparation <esp>\n",
        "        in_enc.append(char_vocab[ESP_CHAR])\n",
        "        # Pour la sortie, récupérer le trait Number (sinon, \"<N/A>\")\n",
        "        feats = token.get(\"feats\")\n",
        "        if feats and \"Number\" in feats:\n",
        "            out_val = num_vocab.get(feats[\"Number\"], num_vocab[\"<N/A>\"])\n",
        "        else:\n",
        "            out_val = num_vocab[\"<N/A>\"]\n",
        "        out_enc.append(out_val)\n",
        "    # Si la séquence de caractères est trop longue, on la tronque et on retire les mots dont la fin dépasse max_c\n",
        "    if len(in_enc) > max_c:\n",
        "        in_enc = in_enc[:max_c]\n",
        "        ends = [e for e in ends if e < max_c]\n",
        "        out_enc = out_enc[:len(ends)]\n",
        "    # Tronquer le nombre de mots à max_w\n",
        "    if len(ends) > max_w:\n",
        "        ends = ends[:max_w]\n",
        "        out_enc = out_enc[:max_w]\n",
        "    return in_enc, ends, out_enc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# DATASET ET DATA LOADER\n",
        "########################################\n",
        "\n",
        "class MorphDataset(Dataset):\n",
        "    def __init__(self, data_file, char_vocab, num_vocab, max_c, max_w):\n",
        "        self.samples = []\n",
        "        with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            for sentence in parse_incr(f):\n",
        "                in_enc, ends, out_enc = encode_sentence(sentence, char_vocab, num_vocab, max_c, max_w)\n",
        "                self.samples.append((in_enc, ends, out_enc))\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "def collate_fn(samples):\n",
        "    \"\"\"\n",
        "    Rassemble une liste d'exemples et applique le padding pour obtenir des tenseurs de même taille.\n",
        "    Chaque exemple est un tuple (in_enc, ends, out_enc).\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    max_c = max(len(s[0]) for s in samples)\n",
        "    max_w = max(len(s[1]) for s in samples)\n",
        "    batch_in, batch_ends, batch_out = [], [], []\n",
        "    for in_enc, ends, out_enc in samples:\n",
        "        in_enc_tensor = torch.tensor(in_enc + [PAD_ID]*(max_c - len(in_enc)), dtype=torch.long)\n",
        "        ends_tensor = torch.tensor(ends + [0]*(max_w - len(ends)), dtype=torch.long)\n",
        "        out_enc_tensor = torch.tensor(out_enc + [0]*(max_w - len(out_enc)), dtype=torch.long)\n",
        "        batch_in.append(in_enc_tensor)\n",
        "        batch_ends.append(ends_tensor)\n",
        "        batch_out.append(out_enc_tensor)\n",
        "    return torch.stack(batch_in), torch.stack(batch_ends), torch.stack(batch_out)\n",
        "\n",
        "########################################\n",
        "# DÉFINITION DU MODÈLE\n",
        "########################################\n",
        "\n",
        "class CharMorphTagger(nn.Module):\n",
        "    def __init__(self, char_vocab_size, num_classes, embedding_dim, hidden_dim, padding_idx=PAD_ID, dropout=0.5):\n",
        "        super(CharMorphTagger, self).__init__()\n",
        "        self.embedding = nn.Embedding(char_vocab_size, embedding_dim, padding_idx=padding_idx)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "        self.hidden_dim = hidden_dim\n",
        "    def forward(self, in_enc, ends):\n",
        "        \"\"\"\n",
        "        in_enc : tenseur Long de taille (B, max_c)\n",
        "        ends   : tenseur Long de taille (B, max_w) indiquant pour chaque mot l'index dans in_enc de son dernier caractère.\n",
        "        \"\"\"\n",
        "        embeds = self.embedding(in_enc)         # (B, max_c, embedding_dim)\n",
        "        gru_out, _ = self.gru(embeds)             # (B, max_c, hidden_dim)\n",
        "        # Préparer ends pour gather : on ajoute une dimension pour hidden_dim\n",
        "        ends_exp = ends.unsqueeze(2).expand(-1, -1, self.hidden_dim)  # (B, max_w, hidden_dim)\n",
        "        # Récupérer les états cachés correspondant aux fins de mots\n",
        "        word_reps = torch.gather(gru_out, 1, ends_exp)  # (B, max_w, hidden_dim)\n",
        "        word_reps = self.dropout(word_reps)\n",
        "        logits = self.fc(word_reps)             # (B, max_w, num_classes)\n",
        "        return logits\n",
        "\n",
        "########################################\n",
        "# FONCTIONS D'ENTRAÎNEMENT & D'ÉVALUATION\n",
        "########################################\n",
        "\n",
        "def train_model(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch_in, batch_ends, batch_out in dataloader:\n",
        "        batch_in = batch_in.to(device)\n",
        "        batch_ends = batch_ends.to(device)\n",
        "        batch_out = batch_out.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_in, batch_ends)  # (B, max_w, num_classes)\n",
        "        loss = criterion(outputs.view(-1, outputs.shape[-1]), batch_out.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total_tokens = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_in, batch_ends, batch_out in dataloader:\n",
        "            batch_in = batch_in.to(device)\n",
        "            batch_ends = batch_ends.to(device)\n",
        "            batch_out = batch_out.to(device)\n",
        "            outputs = model(batch_in, batch_ends)  # (B, max_w, num_classes)\n",
        "            loss = criterion(outputs.view(-1, outputs.shape[-1]), batch_out.view(-1))\n",
        "            total_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=-1)\n",
        "            mask = batch_out != 0  # on ignore le padding (<PAD> a l'indice 0)\n",
        "            correct += (preds[mask] == batch_out[mask]).sum().item()\n",
        "            total_tokens += mask.sum().item()\n",
        "    accuracy = correct / total_tokens if total_tokens > 0 else 0\n",
        "    return total_loss / len(dataloader), accuracy\n"
      ],
      "metadata": {
        "id": "Vma5B1S1772A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# PARAMÈTRES & CHARGEMENT DES DONNÉES\n",
        "########################################\n",
        "\n",
        "# Chemins vers vos fichiers de données\n",
        "train_file = \"/content/qaf_arabizi-ud-train.conllu\"\n",
        "dev_file   = \"/content/qaf_arabizi-ud-dev.conllu\"\n",
        "\n",
        "# Paramètres d'encodage\n",
        "max_c = 200   # nombre maximum de caractères par phrase\n",
        "max_w = 20    # nombre maximum de mots par phrase\n",
        "\n",
        "# Construction des vocabulaires\n",
        "char_vocab = build_char_vocab(train_file, min_freq=1)\n",
        "num_vocab = build_number_vocab(train_file)\n",
        "print(\"Taille du vocabulaire des caractères :\", len(char_vocab))\n",
        "print(\"Vocabulaire Number :\", num_vocab)\n",
        "\n",
        "# Création des datasets et DataLoaders\n",
        "train_dataset = MorphDataset(train_file, char_vocab, num_vocab, max_c, max_w)\n",
        "dev_dataset = MorphDataset(dev_file, char_vocab, num_vocab, max_c, max_w)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "dev_loader   = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "########################################\n",
        "# INITIALISATION DU MODÈLE, OPTIMISEUR, ETC.\n",
        "########################################\n",
        "\n",
        "embedding_dim = 50\n",
        "hidden_dim = 100\n",
        "dropout = 0.5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CharMorphTagger(char_vocab_size=len(char_vocab), num_classes=len(num_vocab),\n",
        "                        embedding_dim=embedding_dim, hidden_dim=hidden_dim,\n",
        "                        padding_idx=PAD_ID, dropout=dropout)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # on ignore les pads (indice 0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "########################################\n",
        "# BOUCLE D'ENTRAÎNEMENT\n",
        "########################################\n",
        "\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
        "    dev_loss, dev_acc = evaluate_model(model, dev_loader, criterion, device)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {train_loss:.4f} | Dev Loss = {dev_loss:.4f} | Dev Acc = {dev_acc:.4f}\")\n",
        "\n",
        "# Optionnel : sauvegarder le modèle et les vocabulaires\n",
        "checkpoint = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'char_vocab': char_vocab,\n",
        "    'num_vocab': num_vocab,\n",
        "    'embedding_dim': embedding_dim,\n",
        "    'hidden_dim': hidden_dim,\n",
        "    'max_c': max_c,\n",
        "    'max_w': max_w\n",
        "}\n",
        "torch.save(checkpoint, \"/content/morph_tagger.pt\")\n",
        "print(\"Modèle sauvegardé dans morph_tagger.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jthjjh-q8S8B",
        "outputId": "ae1e62a9-72b1-4708-f6fd-891f960d91d3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille du vocabulaire des caractères : 115\n",
            "Vocabulaire Number : {'<PAD>': 0, '<N/A>': 1, 'Sing': 2, 'Plur': 3}\n",
            "Epoch 1/5: Train Loss = 0.6386 | Dev Loss = 0.3521 | Dev Acc = 0.9206\n",
            "Epoch 2/5: Train Loss = 0.3421 | Dev Loss = 0.3120 | Dev Acc = 0.9196\n",
            "Epoch 3/5: Train Loss = 0.3085 | Dev Loss = 0.2748 | Dev Acc = 0.9206\n",
            "Epoch 4/5: Train Loss = 0.2608 | Dev Loss = 0.2326 | Dev Acc = 0.9319\n",
            "Epoch 5/5: Train Loss = 0.2308 | Dev Loss = 0.2128 | Dev Acc = 0.9401\n",
            "Modèle sauvegardé dans morph_tagger.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score # Import necessary functions\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from conllu import parse_incr\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "def evaluate_metrics(model, dataloader, device, UNK_ID, PAD_ID=0):\n",
        "    model.eval()\n",
        "    all_preds, all_tags = [], []\n",
        "    oov_preds, oov_tags = [], []\n",
        "    known_preds, known_tags = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_in, batch_ends, batch_out in dataloader:\n",
        "            batch_in = batch_in.to(device)\n",
        "            batch_ends = batch_ends.to(device) # Move batch_ends to device\n",
        "            batch_out = batch_out.to(device)\n",
        "            outputs = model(batch_in, batch_ends)\n",
        "            predictions = outputs.argmax(dim=-1)\n",
        "\n",
        "            # Parcourir chaque batch et chaque token\n",
        "            for i in range(batch_out.shape[0]):  # Use batch_out.shape[0] for batch size\n",
        "                for j in range(batch_out.shape[1]):  # Use batch_out.shape[1] for word sequence length\n",
        "                    # Ignorer le padding\n",
        "                    if batch_out[i, j].item() == PAD_ID:\n",
        "                        continue\n",
        "                    pred = predictions[i, j].item()\n",
        "                    true = batch_out[i, j].item()\n",
        "                    all_preds.append(pred)\n",
        "                    all_tags.append(true)\n",
        "\n",
        "                    # Vérifier si le token est OOV en utilisant batch_in et ends\n",
        "                    # Get the character index for the current word\n",
        "                    if j < len(batch_ends[i]) and batch_ends[i][j] != 0:\n",
        "                      word_end_index = batch_ends[i][j].item()\n",
        "                      # Check if any character in the word is UNK\n",
        "                      is_oov = any(batch_in[i, k].item() == UNK_ID for k in range(word_end_index))\n",
        "                    else:\n",
        "                      is_oov = False\n",
        "\n",
        "                    #is_oov = batch_in[i, batch_ends[i][j] if j < len(batch_ends[i]) else 0].item() == UNK_ID\n",
        "                    if is_oov:  # Si le token est OOV\n",
        "                        oov_preds.append(pred)\n",
        "                        oov_tags.append(true)\n",
        "                    else:\n",
        "                        known_preds.append(pred)\n",
        "                        known_tags.append(true)\n",
        "\n",
        "    overall_acc = accuracy_score(all_tags, all_preds)\n",
        "    overall_f1 = f1_score(all_tags, all_preds, average=\"macro\")\n",
        "\n",
        "    known_acc = accuracy_score(known_tags, known_preds) if known_tags else 0.0\n",
        "    known_f1 = f1_score(known_tags, known_preds, average=\"macro\") if known_tags else 0.0\n",
        "\n",
        "    oov_acc = accuracy_score(oov_tags, oov_preds) if oov_tags else 0.0\n",
        "    oov_f1 = f1_score(oov_tags, oov_preds, average=\"macro\") if oov_tags else 0.0\n",
        "\n",
        "    return {\n",
        "        \"overall_accuracy\": overall_acc,\n",
        "        \"overall_f1\": overall_f1,\n",
        "        \"known_accuracy\": known_acc,\n",
        "        \"known_f1\": known_f1,\n",
        "        \"oov_accuracy\": oov_acc,\n",
        "        \"oov_f1\": oov_f1\n",
        "    }"
      ],
      "metadata": {
        "id": "8rZZqZkM_Gmb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, epochs+1):\n",
        "    train_loss = train_model(model, train_loader, criterion, optimizer, device) # Change 'train' to 'train_model'\n",
        "    dev_loss, dev_accuracy = evaluate_model(model, dev_loader, criterion, device) # Change 'evaluate' to 'evaluate_model'\n",
        "\n",
        "    # Calcul des métriques détaillées (accuracy et F1 global, sur tokens connus et OOV)\n",
        "    metrics = evaluate_metrics(model, dev_loader, device, UNK_ID, PAD_ID)\n",
        "\n",
        "    print(f\"Epoch {epoch}: train loss = {train_loss:.4f}, dev loss = {dev_loss:.4f}, dev accuracy = {dev_accuracy:.4f}\")\n",
        "    print(\"Overall Accuracy: {:.4f}, Overall F1: {:.4f}\".format(metrics[\"overall_accuracy\"], metrics[\"overall_f1\"]))\n",
        "    print(\"Known Accuracy: {:.4f}, Known F1: {:.4f}\".format(metrics[\"known_accuracy\"], metrics[\"known_f1\"]))\n",
        "    print(\"OOV Accuracy: {:.4f}, OOV F1: {:.4f}\".format(metrics[\"oov_accuracy\"], metrics[\"oov_f1\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_53EPrFD9osN",
        "outputId": "9d7f9b63-c607-4abe-e586-34aa609dd2ea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train loss = 0.2176, dev loss = 0.2051, dev accuracy = 0.9427\n",
            "Overall Accuracy: 0.9427, Overall F1: 0.6441\n",
            "Known Accuracy: 0.9426, Known F1: 0.6452\n",
            "OOV Accuracy: 0.9444, OOV F1: 0.4857\n",
            "Epoch 2: train loss = 0.2083, dev loss = 0.1981, dev accuracy = 0.9411\n",
            "Overall Accuracy: 0.9411, Overall F1: 0.6368\n",
            "Known Accuracy: 0.9411, Known F1: 0.6379\n",
            "OOV Accuracy: 0.9444, OOV F1: 0.4857\n",
            "Epoch 3: train loss = 0.1992, dev loss = 0.1935, dev accuracy = 0.9427\n",
            "Overall Accuracy: 0.9427, Overall F1: 0.6565\n",
            "Known Accuracy: 0.9426, Known F1: 0.6576\n",
            "OOV Accuracy: 0.9444, OOV F1: 0.4857\n",
            "Epoch 4: train loss = 0.1972, dev loss = 0.1937, dev accuracy = 0.9442\n",
            "Overall Accuracy: 0.9442, Overall F1: 0.6487\n",
            "Known Accuracy: 0.9437, Known F1: 0.6486\n",
            "OOV Accuracy: 1.0000, OOV F1: 1.0000\n",
            "Epoch 5: train loss = 0.1922, dev loss = 0.1909, dev accuracy = 0.9442\n",
            "Overall Accuracy: 0.9442, Overall F1: 0.6541\n",
            "Known Accuracy: 0.9437, Known F1: 0.6540\n",
            "OOV Accuracy: 1.0000, OOV F1: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from conllu import parse_incr\n",
        "\n",
        "\n",
        "def load_model(model_path, device):\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    char_vocab = checkpoint['char_vocab']\n",
        "    num_vocab = checkpoint['num_vocab']\n",
        "    embedding_dim = checkpoint['embedding_dim']\n",
        "    hidden_dim = checkpoint['hidden_dim']\n",
        "    max_c = checkpoint['max_c']\n",
        "    max_w = checkpoint['max_w']\n",
        "    # Instanciation du modèle (la classe CharMorphTagger doit être définie dans le notebook)\n",
        "    model = CharMorphTagger(char_vocab_size=len(char_vocab), num_classes=len(num_vocab),\n",
        "                            embedding_dim=embedding_dim, hidden_dim=hidden_dim,\n",
        "                            padding_idx=0, dropout=0.5)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model, char_vocab, num_vocab, max_c, max_w\n",
        "\n",
        "def predict_sentence(model, sentence, char_vocab, num_vocab, device, max_c, max_w):\n",
        "    # encode_sentence est la fonction qui transforme une phrase en (in_enc, ends, out_enc)\n",
        "    in_enc, ends, _ = encode_sentence(sentence, char_vocab, num_vocab, max_c, max_w)\n",
        "    in_tensor = torch.tensor(in_enc, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    ends_tensor = torch.tensor(ends, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(in_tensor, ends_tensor)  # (1, max_w, num_classes)\n",
        "        preds = outputs.argmax(dim=-1).squeeze(0).tolist()\n",
        "    # Créer un dictionnaire inverse pour num_vocab\n",
        "    rev_num_vocab = {v: k for k, v in num_vocab.items()}\n",
        "    predicted_labels = [rev_num_vocab[p] for p in preds]\n",
        "    return predicted_labels\n",
        "\n",
        "# ---- Chargement du modèle et prédiction sur le fichier test ----\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model, char_vocab, num_vocab, max_c, max_w = load_model(\"/content/morph_tagger.pt\", device)\n",
        "\n",
        "with open(\"/content/pred.conllu\", \"w\", encoding=\"utf-8\") as outf:\n",
        "    with open(\"/content/qaf_arabizi-ud-test.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
        "        for sentence in parse_incr(f):\n",
        "            # Prédire le trait Number pour chaque token de la phrase\n",
        "            predicted_number = predict_sentence(model, sentence, char_vocab, num_vocab, device, max_c, max_w)\n",
        "            # Mettre à jour le champ \"feats\" : si la prédiction est \"<N/A>\", on met None ; sinon, on remplace par {\"Number\": tag}\n",
        "            for token, tag in zip(sentence, predicted_number):\n",
        "                if tag == \"<N/A>\":\n",
        "                    token[\"feats\"] = None\n",
        "                else:\n",
        "                    token[\"feats\"] = {\"Number\": tag}\n",
        "            outf.write(sentence.serialize())\n",
        "            outf.write(\"\\n\")\n",
        "\n",
        "# ---- Lancement de l'évaluation avec accuracy.py ----\n",
        "!python accuracy.py -p pred.conllu -g qaf_arabizi-ud-test.conllu -t qaf_arabizi-ud-train.conllu -c feats -f form\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoaUC6Ck_o21",
        "outputId": "5132954d-7e1d-45dd-e653-7693cfaf9855"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-8728e6d9fd95>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(model_path, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions file: pred.conllu\n",
            "Accuracy on all feats: 79.18 ( 1818/ 2296)\n",
            "Accuracy on OOV feats: 76.35 (  607/  795)\n",
            "\n",
            "Precision, recall, and F-score for feats:\n",
            "AdpType    : P=100.00 (   32/   32) / R= 17.20 (   32/  186) / F= 29.36\n",
            "Gender     : P=100.00 (   36/   36) / R= 16.90 (   36/  213) / F= 28.92\n",
            "Mood       : P=100.00 (    7/    7) / R= 14.89 (    7/   47) / F= 25.93\n",
            "Number     : P= 74.38 (   90/  121) / R= 42.06 (   90/  214) / F= 53.73\n",
            "Person     : P=100.00 (   36/   36) / R= 16.82 (   36/  214) / F= 28.80\n",
            "Polarity   : P=100.00 (    6/    6) / R= 10.91 (    6/   55) / F= 19.67\n",
            "PronType   : P=100.00 (    4/    4) / R= 10.26 (    4/   39) / F= 18.60\n",
            "Typo       : P=100.00 (    4/    4) / R= 36.36 (    4/   11) / F= 53.33\n",
            "VerbForm   : P=100.00 (    3/    3) / R= 20.00 (    3/   15) / F= 33.33\n",
            "\n",
            "micro-avg  : P= 87.55 (  218/  249) / R= 21.93 (  218/  994) / F= 35.08\n",
            "macro-avg  : P= 97.15               / R= 20.60               / F= 33.99\n"
          ]
        }
      ]
    }
  ]
}